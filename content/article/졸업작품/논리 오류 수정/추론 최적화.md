---
date: '2025-09-21T16:39:08+09:00'
draft: false
title: '추론 최적화'
weight: 7
---

기존 학습 코드를 최소 수정으로 추론 모듈로 만들었었다.  
이번에는 이를 **추론에 최적화된 형태**로 리팩터링하고, 인터넷 연결이 필요 없는 단일 이미지 형태로 패키징했다.

이로써 추론 **속도 37% 단축, 처리량 45% 증가, 에너지 효율 53% 개선**이 이루어졌으며  
서버리스 환경에서도 **인터넷 연결 없이 곧바로 사용**할 수 있게 되었다.

## 실행 모듈 구조

`wpc.py`는 모델을 로딩하고 입력된 코드를 변환하여 추론을 수행하는 핵심 모듈이다.
여기서 중요한 부분은 **입력 피처 변환** 과정이다.

### 개선 전

```python {base_url="https://github.com/PDA-PRO/COCO_AI/blob/main/wpc/reference/",filename="wpc.py"}
def process(self, code, p_id):
    ...
    eval_examples = [self.Example(...)]
    eval_features = self.convert_examples_to_features(eval_examples, self.tokenizer)
    eval_data = self.TextDataset(eval_features)
    eval_dataloader = DataLoader(eval_data, batch_size=64, num_workers=4)
    ...
```

단일 추론임에도 불구하고 굳이 `DataLoader`를 거쳐서 피처를 생성하고 있었다.

### 개선 후

```python {base_url="https://github.com/PDA-PRO/COCO_AI/blob/main/wpc/reference/",filename="wpc.py"}
def process(self, code, p_id):
    ...
    feature = self.convert_to_feature(abstracted_code, p_id)
    if feature is None:
        return None

    source_ids, source_mask, position_idx, attn_mask = self._prepare_batch_from_feature(feature)

    with torch.inference_mode():
        preds = self.model(source_ids, source_mask, position_idx, attn_mask)
    ...
```

데이터로더 기반 흐름을 제거하고 **단일 피처 변환** 방식으로 단순화했다.
또한 입력 피처 구조체에서도 추론에 필요 없는 항목들을 제거했다:

```diff {base_url="https://github.com/PDA-PRO/COCO_AI/blob/main/wpc/reference/",filename="wpc.py"}
class InputFeatures(object):
-    example_id
     source_ids
     position_idx
     dfg_to_code
     dfg_to_dfg
-    target_ids
     source_mask
-    target_mask
```



## `torch.compile` 적용

모델(`model.py`)에 `torch.compile()`을 적용했다.  
다만 `beam_search`가 파이썬 구현이라 디코드 부분만 따로 적용했다.

## 단일 파일 번들링

추론 자체는 인터넷을 필요로 하지 않지만,  
초기 로딩 시 HuggingFace Hub에서 **GraphCodeBERT 설정값**을 불러오려 인터넷 연결이 발생했다.  
서버리스 환경에서는 이 오버헤드가 불필요하다고 판단, 아예 **모델+토크나이저+config**를 단일 `.pt` 파일로 번들링했다.

### 번들링 함수

```python {base_url="https://github.com/PDA-PRO/COCO_AI/blob/main/wpc/reference/",filename="pack_signlefile.py"}
def save_all_to_single_pt(model, tokenizer, config, out_path, meta=None, hf_id="microsoft/graphcodebert-base"):
    ...
    payload = {
        "format": "wpc_bundle_v1",
        "hf_id": hf_id,
        "config": config.to_dict(),
        "tokenizer_files": tok_files,
        "state_dict": model.state_dict(),
        "meta": meta or {},
    }
    torch.save(payload, out_path)
```

### 결과

```powershell
Mode                 LastWriteTime         Length Name
----                 -------------         ------ ----
-a-            2025-09-19  오후 11:37 692790260 wpc_bundle.pt
```

완성된 번들(`wpc_bundle.pt`)을 로딩하는 방식으로 교체하여, 인터넷 연결 없이도 추론 모듈이 작동하게 되었다.



## 성능 검증

테스트 환경:

* GPU 1개 (실시간 추론 환경)
* Docker 컨테이너: CPU 8코어 / 메모리 16GiB 할당
* 테스트 데이터: 학습 데이터 중 무작위 1000개 추출
* Warm-up 200건 후, 1000건 추론
* 리소스 측정: `docker stats`, `nvidia-smi dmon`

### 결과 요약

| 지표                  |      Before |          After |         변화율 |
| - | -: | -: | -: |
| 처리량 (jobs/s)        |        0.38 |       **0.55** |      +44.7% |
| 총 소요(1000건, s)      |      2658.7 |     **1814.3** |      −31.8% |
| p50 지연 (ms)         |        1984 |       **1238** |      −37.6% |
| p99 지연 (ms)         |        5531 |       **3488** |      −36.9% |
| CPU 평균 (%)          |       108.9 |      **100.0** |       −8.2% |
| 메모리 평균 (GiB)        |        1.91 |       **2.28** |      +19.4% |
| GPU 전력 평균 / 최대 (W)  | 133.1 / 153 | **89.5 / 120** | −33% / −22% |
| GPU 온도 평균 / 최대 (°C) |   67.0 / 72 |  **55.4 / 62** | −17% / −14% |

### 에너지 효율

* Before: 133.1W ÷ 0.38 ≈ **350 J/건**
* After: 89.5W ÷ 0.55 ≈ **163 J/건**
* → **건당 에너지 53.5% 절감**, RPS/W는 **+115%**

## 해석

1. **성능과 효율 동시 개선**

   * 처리량 +44.7%, 지연시간 −35% 수준.
   * 전력/온도 모두 낮아진 점에서 불필요 연산 제거 및 GPU 커널 최적화 효과로 해석된다.

2. **GPU 활용률 감소, 그러나 처리량 증가**

   * 평균 util이 줄었음에도 성능이 상승 → **작업 시간이 짧아져 GPU가 빨리 놀게 된 것**.

3. **CPU는 병목 아님**

   * CPU 사용률은 크게 변하지 않았고 GPU 최적화가 주요한 개선 포인트였다.



## 마무리

이번 리팩터링과 torch.compile 적용, 번들링 작업으로 **성능·효율·안정성**이 개선됨을 확인했다.  
다음 시간에는 서버리스 추론을 적용해보도록 하겠다.

